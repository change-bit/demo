from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, Distance
import torch

torch.cuda.empty_cache()  # é‡Šæ”¾æ˜¾å­˜ç¢ç‰‡ï¼Œå¿…å¤‡

#å‘é‡åº“é…ç½®
EMBEDDING_DIM = 1536
COLLECTION_NAME = "full_demo"
PATH = "./qdrant_db"
client = QdrantClient(path=PATH, allow_concurrent_reads=True)

#  ä¾èµ–å¯¼å…¥
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core import Settings
from llama_index.embeddings.dashscope import DashScopeEmbedding
from llama_index.core.postprocessor import SimilarityPostprocessor

# åŠ è½½æœ¬åœ°æ¨¡å‹+ç»‘å®šå…¨å±€Settings
from transformers import AutoModelForCausalLM, AutoTokenizer
import warnings

warnings.filterwarnings('ignore')  # å…³é—­æ— å…³æ—¥å¿—

MODEL_PATH = r"F:\llmv1.0\inori"
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
# ä¿®å¤torch_dtypeè­¦å‘Šï¼štorch_dtype â†’ dtype
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    dtype=torch.float16,
    device_map="cuda:0",
    trust_remote_code=True,
    low_cpu_mem_usage=True
)

# ç»‘å®štokené…ç½®ï¼Œç¨³å®šç”Ÿæˆ
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = tokenizer.eos_token_id
model.eval()

# å®šä¹‰CustomLLMç±»ï¼Œç»‘å®šå…¨å±€
from llama_index.core.llms import CustomLLM, CompletionResponse, CompletionResponseGen
from typing import Any, List
import types


class CustomDeepSeekLLM(CustomLLM):
    @property
    def metadata(self):
        meta = types.SimpleNamespace()
        meta.model_name = "inori-DeepSeek1.5B"
        meta.context_window = 4096
        meta.num_output = 768
        meta.is_chat_model = True
        return meta

    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:
        prompt = f"<ï½œbeginâ–ofâ–sentenceï½œ>{prompt}\n"
        inputs = tokenizer(prompt, return_tensors="pt").to("cuda:0")
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=768,
                temperature=0.25,
                top_p=0.85,
                do_sample=True,
                repetition_penalty=1.1,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.pad_token_id
            )
        answer = tokenizer.decode(outputs[0][len(inputs["input_ids"][0]):], skip_special_tokens=True)
        return CompletionResponse(text=answer)

    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:
        yield self.complete(prompt, **kwargs)


Settings.llm = CustomDeepSeekLLM()

# é…ç½®Embedding+æ–‡æ¡£åˆ†ç‰‡
Settings.embed_model = DashScopeEmbedding(
    model_name="text-embedding-v1",
    api_key="sk-2231d947be55426fb8ebb3057c2b7072"
)
# ä¸­æ–‡æ–‡æ¡£æœ€ä¼˜åˆ†ç‰‡é…ç½®ï¼Œæ— å†—ä½™
Settings.transformations = [SentenceSplitter(chunk_size=300, chunk_overlap=50)]

#åŠ è½½æ–‡æ¡£
documents = SimpleDirectoryReader("./data").load_data()

# åˆå§‹åŒ–å‘é‡åº“
if client.collection_exists(COLLECTION_NAME):
    client.delete_collection(COLLECTION_NAME)
client.create_collection(COLLECTION_NAME, vectors_config=VectorParams(size=EMBEDDING_DIM, distance=Distance.COSINE))
vector_store = QdrantVectorStore(client=client, collection_name=COLLECTION_NAME)
storage_context = StorageContext.from_defaults(vector_store=vector_store)
index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)


# ç›¸ä¼¼åº¦é˜ˆå€¼0.4ï¼ˆè¿‡é«˜ä¼šæ£€ç´¢ä¸åˆ°ï¼‰
sp = SimilarityPostprocessor(similarity_cutoff=0.4)

retriever = index.as_retriever(similarity_top_k=5)


# Prompt
def build_prompt(question, retrieved_text):
    if retrieved_text:
        # å¦‚æœæœ‰çŸ¥è¯†åº“ï¼Œå¼ºåˆ¶å‚è€ƒçŸ¥è¯†åº“å›ç­”
        prompt = f"""<ï½œbeginâ–ofâ–sentenceï½œ>
ä¸¥æ ¼æŒ‰ç…§ã€å·²çŸ¥ä¿¡æ¯ã€‘å›ç­”é—®é¢˜ï¼Œå·²çŸ¥ä¿¡æ¯æ˜¯å”¯ä¸€ç­”æ¡ˆæ¥æºï¼Œç¦æ­¢ç¼–é€ å†…å®¹ã€ç¦æ­¢è¯´æ— å…³çš„è¯ã€‚
ç²¾å‡†æç‚¼å·²çŸ¥ä¿¡æ¯ä¸­çš„ç­”æ¡ˆå³å¯ï¼Œä¸è¦æ·»åŠ é¢å¤–å†…å®¹ã€‚

ã€å·²çŸ¥ä¿¡æ¯ã€‘ï¼š
{retrieved_text}

ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š{question}
ã€å›ç­”ã€‘ï¼š"""
    else:
        # å¦‚æœæ²¡æœ‰çŸ¥è¯†åº“ï¼Œå…è®¸æ¨¡å‹ç”¨è‡ªå·±çš„çŸ¥è¯†è‡ªç”±å›ç­”ï¼ˆæ¢å¤èŠå¤©åŠŸèƒ½ï¼‰
        prompt = f"""<ï½œbeginâ–ofâ–sentenceï½œ>
ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š{question}
ã€å›ç­”ã€‘ï¼š"""
    return prompt


# ç”Ÿæˆå›ç­”
def generate_answer(prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda:0")
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=768,
            temperature=0.25,
            top_p=0.85,
            do_sample=True,
            repetition_penalty=1.1,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id
        )
    answer = tokenizer.decode(outputs[0][len(inputs["input_ids"][0]):], skip_special_tokens=True)
    return answer


# ===================== 8. äº¤äº’å¾ªç¯+è°ƒè¯•æ—¥å¿—+é€€å‡ºæç¤º====================
print("=====================inori-DeepSeek1.5B çº¯æœ¬åœ°RAG =====================")
print("=====================è¾“å…¥ exit/quit/é€€å‡º ç»“æŸå¯¹è¯   =====================\n")

while True:
    question = input("User: ")
    if question.strip() in ["exit", "quit", "é€€å‡º"]:
        print("\nğŸ‰ é€€å‡ºæˆåŠŸï¼Œæ¥ªç¥ˆæ°¸è¿œé™ªä¼´ä½  â¤ï¸ ğŸ‰")
        break

    # æ£€ç´¢æµç¨‹ï¼šæç®€æ— å†—ä½™ï¼Œå¿…å‡ºç»“æœ
    retrieved_nodes = retriever.retrieve(question)
    filtered_nodes = sp.postprocess_nodes(retrieved_nodes)
    retrieved_text = "\n".join([node.text for node in filtered_nodes])

    # è°ƒè¯•æ—¥å¿—ï¼šæŸ¥çœ‹æ£€ç´¢ç»“æœï¼ˆå¯éšæ—¶åˆ æ‰ï¼Œä¸å½±å“åŠŸèƒ½ï¼‰
    print("\n" + "=" * 60)
    print("ğŸ“„ æ£€ç´¢åˆ°çš„çŸ¥è¯†åº“å†…å®¹")
    print(retrieved_text if retrieved_text else "âŒ æš‚æ— åŒ¹é…å†…å®¹")
    print("=" * 60 + "\n")

    # ç”Ÿæˆå›ç­”
    prompt = build_prompt(question, retrieved_text)
    answer = generate_answer(prompt)

    print(f"AI: {answer}\n")